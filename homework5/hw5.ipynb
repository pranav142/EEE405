{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stdev:\n",
      "[[0.34894699 0.51098337 0.62948868]\n",
      " [0.37525458 0.31064449 0.31925538]\n",
      " [0.17191859 0.46518813 0.54634787]\n",
      " [0.10432641 0.19576517 0.27188968]]\n",
      "\n",
      "Mu:\n",
      "[[5.006 5.936 6.588]\n",
      " [3.428 2.77  2.974]\n",
      " [1.462 4.26  5.552]\n",
      " [0.246 1.326 2.026]]\n"
     ]
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data[:,0:4]\n",
    "y = iris.target\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "num_classes = len(np.unique(y))\n",
    "num_features = X.shape[1]\n",
    "\n",
    "s = np.zeros((num_features, num_classes))\n",
    "u = np.zeros((num_features, num_classes))\n",
    "\n",
    "def print_cm(y_combined, y_pred):\n",
    "    cm = confusion_matrix(y_combined, y_pred, labels=[0, 1, 2])\n",
    "    print(' number in each class down vs number in each known class across ')\n",
    "    print(' confusion matrix \\n 0 1 2\\n', cm.T) ## transpose\n",
    "\n",
    "def PGauss(mu, sig, x):\n",
    "    return np.exp(-np.power(x - mu, 2.) / (2 * np.power(sig, 2.) + 1e-300) )\n",
    "\n",
    "for c in range(num_classes): \n",
    "    for f in range(num_features):\n",
    "        u[f, c] = X[np.where(y==c), f].mean()\n",
    "        s[f, c] = X[np.where(y==c), f].std()\n",
    " \n",
    "print(\"stdev:\")\n",
    "print(s)\n",
    "\n",
    "print(\"\\nMu:\")\n",
    "print(u)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "priors = np.zeros(num_classes)\n",
    "for c in range(num_classes):\n",
    "    priors[c] = np.sum(y == c) / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions on all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1\n",
      " 1 1 1 2 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 1 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.zeros_like(y)\n",
    "\n",
    "for i in range(len(X)):\n",
    "    scores = np.zeros(num_classes)\n",
    "    \n",
    "    for c in range(num_classes):\n",
    "        score_c = priors[c]\n",
    "        for f in range(num_features):\n",
    "            score_c *= PGauss(u[f, c], s[f, c], X[i, f]) \n",
    "        scores[c] = score_c\n",
    "\n",
    "    y_pred[i] = np.argmax(scores)\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in train: 105\n",
      "Number of samples in test: 45\n",
      "Number features 4\n",
      "\n",
      "TEST RESULTS\n",
      "Number of misclassifications: 4\n",
      "Accuracy: 91.11%\n",
      "Indices of misclassified samples: [ 2  3 39 42]\n",
      "Actual labels: [2 1 1 2]\n",
      "Predicted labels: [1 2 2 1]\n",
      " number in each class down vs number in each known class across \n",
      " confusion matrix \n",
      " 0 1 2\n",
      " [[15  0  0]\n",
      " [ 0 13  2]\n",
      " [ 0  2 13]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.zeros_like(y_test)\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    scores = np.zeros(num_classes)\n",
    "    \n",
    "    for c in range(num_classes):\n",
    "        score_c = priors[c]\n",
    "        for f in range(num_features):\n",
    "            score_c *= PGauss(u[f, c], s[f, c], X_test[i, f]) \n",
    "        scores[c] = score_c\n",
    "\n",
    "    y_pred[i] = np.argmax(scores)\n",
    "\n",
    "misclassified_indices = np.where(y_test != y_pred)[0]\n",
    "num_misclassified = len(misclassified_indices)\n",
    "accuracy = 1.0 - (num_misclassified / len(y_test))\n",
    "\n",
    "print(f\"Number of samples in train: {len(X_train)}\")\n",
    "print(f\"Number of samples in test: {len(X_test)}\")\n",
    "print(f\"Number features {num_features}\")\n",
    "\n",
    "print(\"\\nTEST RESULTS\")\n",
    "print(\"Number of misclassifications:\", num_misclassified)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "print(\"Indices of misclassified samples:\", misclassified_indices)\n",
    "print(\"Actual labels:\", y_test[misclassified_indices])\n",
    "print(\"Predicted labels:\", y_pred[misclassified_indices])\n",
    "\n",
    "print_cm(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN RESULTS\n",
      "\n",
      "Number of misclassifications : 4\n",
      "Accuracy : 96.19%\n",
      "Indices of misclassified samples: [19 28 42 63]\n",
      "Actual labels: [1 1 2 1]\n",
      "Predicted labels: [2 2 1 2]\n",
      " number in each class down vs number in each known class across \n",
      " confusion matrix \n",
      " 0 1 2\n",
      " [[35  0  0]\n",
      " [ 0 32  1]\n",
      " [ 0  3 34]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.zeros_like(y_train)\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    scores = np.zeros(num_classes)\n",
    "    \n",
    "    for c in range(num_classes):\n",
    "        score_c = priors[c]\n",
    "        for f in range(num_features):\n",
    "            score_c *= PGauss(u[f, c], s[f, c], X_train[i, f]) \n",
    "        scores[c] = score_c\n",
    "\n",
    "    y_pred[i] = np.argmax(scores)\n",
    "\n",
    "print(\"TRAIN RESULTS\")\n",
    "misclassified_indices = np.where(y_train != y_pred)[0]\n",
    "num_misclassified = len(misclassified_indices)\n",
    "accuracy = 1.0 - (num_misclassified / len(y_train))\n",
    "\n",
    "print(\"\\nNumber of misclassifications :\", num_misclassified)\n",
    "print(\"Accuracy : {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "print(\"Indices of misclassified samples:\", misclassified_indices)\n",
    "print(\"Actual labels:\", y_train[misclassified_indices])\n",
    "print(\"Predicted labels:\", y_pred[misclassified_indices])\n",
    "\n",
    "print_cm(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in train: 105\n",
      "Number of samples in test: 45\n",
      "Number features 4\n",
      "TEST RESULTS\n",
      "\n",
      "Number of misclassifications: 4\n",
      "Accuracy : 91.11%\n",
      "Indices of misclassified samples: [ 2 30 39 42]\n",
      "Actual labels: [2 2 1 2]\n",
      "Predicted labels: [1 1 2 1]\n",
      " number in each class down vs number in each known class across \n",
      " confusion matrix \n",
      " 0 1 2\n",
      " [[15  0  0]\n",
      " [ 0 14  3]\n",
      " [ 0  1 12]]\n",
      "TRAIN RESULTS\n",
      "\n",
      "Number of misclassifications test: 2\n",
      "Accuracy Test: 98.10%\n",
      "Indices of misclassified samples: [19 42]\n",
      "Actual labels: [1 2]\n",
      "Predicted labels: [2 1]\n",
      " number in each class down vs number in each known class across \n",
      " confusion matrix \n",
      " 0 1 2\n",
      " [[35  0  0]\n",
      " [ 0 34  1]\n",
      " [ 0  1 34]]\n"
     ]
    }
   ],
   "source": [
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "misclassified_indices = np.where(y_test != y_pred)[0]\n",
    "num_misclassified = len(misclassified_indices)\n",
    "accuracy = 1.0 - (num_misclassified / len(y_test))\n",
    "\n",
    "print(f\"Number of samples in train: {len(X_train)}\")\n",
    "print(f\"Number of samples in test: {len(X_test)}\")\n",
    "print(f\"Number features {num_features}\")\n",
    "\n",
    "print(\"TEST RESULTS\")\n",
    "print(\"\\nNumber of misclassifications:\", num_misclassified)\n",
    "print(\"Accuracy : {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "print(\"Indices of misclassified samples:\", misclassified_indices)\n",
    "print(\"Actual labels:\", y_test[misclassified_indices])\n",
    "print(\"Predicted labels:\", y_pred[misclassified_indices])\n",
    "\n",
    "print_cm(y_test, y_pred)\n",
    "\n",
    "y_pred = model.predict(X_train)\n",
    "\n",
    "misclassified_indices = np.where(y_train != y_pred)[0]\n",
    "num_misclassified = len(misclassified_indices)\n",
    "accuracy = 1.0 - (num_misclassified / len(y_train))\n",
    "\n",
    "print(\"TRAIN RESULTS\")\n",
    "print(\"\\nNumber of misclassifications test:\", num_misclassified)\n",
    "print(\"Accuracy Test: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "print(\"Indices of misclassified samples:\", misclassified_indices)\n",
    "print(\"Actual labels:\", y_train[misclassified_indices])\n",
    "print(\"Predicted labels:\", y_pred[misclassified_indices])\n",
    "\n",
    "print_cm(y_train, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
